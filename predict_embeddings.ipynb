{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read train/test sets and node embeddings files\n",
    "def readFiles():\n",
    "    \n",
    "    # Read train/test sets\n",
    "    train_nodes = []\n",
    "    train_y = []\n",
    "    \n",
    "    test_nodes = []\n",
    "    test_y = []\n",
    "    \n",
    "    trainFile = \"train.tsv\"\n",
    "    testFile = \"test.tsv\"\n",
    "    \n",
    "    with open(trainFile) as new:\n",
    "        for line in csv.reader(new, delimiter=\"\\t\"):\n",
    "            train_nodes.append((int(line[0]), int(line[1])))\n",
    "            train_y.append(int(line[2]))\n",
    "    \n",
    "    with open(testFile) as new:\n",
    "        for line in csv.reader(new, delimiter=\"\\t\"):\n",
    "            test_nodes.append((int(line[0]), int(line[1])))\n",
    "            test_y.append(int(line[2]))\n",
    "    \n",
    "    # Read node embeddings file\n",
    "    node_embeddings = {} # dict from node ID to embedding\n",
    "    embeddingFile = \"train_edges.emb\"\n",
    "    \n",
    "    index = 0\n",
    "    with open(embeddingFile) as new:\n",
    "        for line in csv.reader(new, delimiter=\" \"):\n",
    "            # Ignore header\n",
    "            if index == 0:\n",
    "                index += 1\n",
    "                continue\n",
    "            \n",
    "            nodeID = int(line[0])\n",
    "            embeddings = np.array([float(val) for val in line[1:]])\n",
    "            \n",
    "            node_embeddings[nodeID] = embeddings\n",
    "            \n",
    "            index += 1\n",
    "    \n",
    "    return train_nodes, train_y, test_nodes, test_y, node_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create train/test set features from embeddings\n",
    "def createFeatures():\n",
    "    \n",
    "    train_nodes, train_y, test_nodes, test_y, node_embeddings = readFiles()\n",
    "    print len(train_nodes), len(train_y), len(test_nodes), len(test_y), len(node_embeddings)\n",
    "    \n",
    "    # Features for each link include combining embeddings of the two nodes:\n",
    "    # Concatenation, Hadamard, Sum, Distance\n",
    "    \n",
    "    feature_types = ['Concatenation', 'Hadamard Product', 'Sum', 'Distance']\n",
    "    train_x = [[] for i in range(len(feature_types))] # list of feature matrix for each type\n",
    "    test_x = [[] for i in range(len(feature_types))]\n",
    "    \n",
    "    # Combine node embeddings for train edges\n",
    "    for node1, node2 in train_nodes:\n",
    "        node1_embed = node_embeddings[node1]\n",
    "        node2_embed = node_embeddings[node2]\n",
    "        \n",
    "        # Concatenate\n",
    "        concat = np.concatenate((node1_embed, node2_embed))\n",
    "        train_x[0].append(concat)\n",
    "        \n",
    "        # Hadamard Product\n",
    "        hadamard = np.multiply(node1_embed, node2_embed)\n",
    "        train_x[1].append(hadamard)\n",
    "        \n",
    "        # Sum\n",
    "        summation = node1_embed + node2_embed\n",
    "        train_x[2].append(summation)\n",
    "        \n",
    "        # Absolute Distance\n",
    "        distance = np.absolute(node1_embed - node2_embed)\n",
    "        train_x[3].append(distance)\n",
    "    \n",
    "    # Combine node embeddings for test edges\n",
    "    for node1, node2 in test_nodes:\n",
    "        node1_embed = node_embeddings[node1]\n",
    "        node2_embed = node_embeddings[node2]\n",
    "        \n",
    "        # Concatenate\n",
    "        concat = np.concatenate((node1_embed, node2_embed))\n",
    "        test_x[0].append(concat)\n",
    "        \n",
    "        # Hadamard Product\n",
    "        hadamard = np.multiply(node1_embed, node2_embed)\n",
    "        test_x[1].append(hadamard)\n",
    "        \n",
    "        # Sum\n",
    "        summation = node1_embed + node2_embed\n",
    "        test_x[2].append(summation)\n",
    "        \n",
    "        # Absolute Distance\n",
    "        distance = np.absolute(node1_embed - node2_embed)\n",
    "        test_x[3].append(distance)\n",
    "    \n",
    "    train_x = [np.array(matrix) for matrix in train_x]\n",
    "    test_x = [np.array(matrix) for matrix in test_x]\n",
    "    \n",
    "    return train_x, train_y, test_x, test_y, feature_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Predict links using various classifier models; for each classifier, predict on each feature type and evaluate\n",
    "def predict():\n",
    "    train_x, train_y, test_x, test_y, feature_types = createFeatures()\n",
    "    \n",
    "    classifiers = ['Logistic Regression', 'Random Forest']\n",
    "    metrics = ['Accuracy', 'Precision', 'Recall', 'F1 Score']\n",
    "    \n",
    "    # list of performance metrics by feature type for each classifier\n",
    "    performance = [[] for i in range(len(classifiers))]\n",
    "    \n",
    "    # Logistic Regression\n",
    "    print \"Logistic Regression\"\n",
    "    for i in range(len(feature_types)):\n",
    "        print i\n",
    "        logReg = LogisticRegression()\n",
    "        \n",
    "        # Train model and predict\n",
    "        logReg.fit(train_x[i], train_y)\n",
    "        predictions = logReg.predict(test_x[i])\n",
    "        \n",
    "        # Evaluate performance metrics\n",
    "        accuracy = sklearn.metrics.accuracy_score(test_y, predictions)\n",
    "        precision = sklearn.metrics.precision_score(test_y, predictions)\n",
    "        recall = sklearn.metrics.recall_score(test_y, predictions)\n",
    "        f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "        \n",
    "        # Record metrics\n",
    "        performance[0].append((accuracy, precision, recall, f1_score))\n",
    "    \n",
    "    # Random Forest\n",
    "    print \"Random Forest\"\n",
    "    for i in range(len(feature_types)):\n",
    "        print i\n",
    "        rf = RandomForestClassifier()\n",
    "        \n",
    "        # Train model and predict\n",
    "        rf.fit(train_x[i], train_y)\n",
    "        predictions = rf.predict(test_x[i])\n",
    "        \n",
    "        # Evaluate performance metrics\n",
    "        accuracy = sklearn.metrics.accuracy_score(test_y, predictions)\n",
    "        precision = sklearn.metrics.precision_score(test_y, predictions)\n",
    "        recall = sklearn.metrics.recall_score(test_y, predictions)\n",
    "        f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "        \n",
    "        # Record metrics\n",
    "        performance[1].append([accuracy, precision, recall, f1_score])\n",
    "    \n",
    "    return classifiers, feature_types, metrics, performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Write performance metrics to output file\n",
    "def writePerformanceOutput():\n",
    "    classifiers, feature_types, metrics, performance = predict()\n",
    "    outputFileName = \"model_performance.txt\"\n",
    "    \n",
    "    with open(outputFileName, 'w') as output:\n",
    "        for i in range(len(classifiers)):\n",
    "            output.write(classifiers[i] + \":\\n\\n\")\n",
    "            for j in range(len(feature_types)):\n",
    "                output.write(feature_types[j] + \":\\n\")\n",
    "                for k in range(len(metrics)):\n",
    "                    output.write(metrics[k] + \": \" + str(performance[i][j][k]) + \"\\n\")\n",
    "                output.write(\"\\n\")\n",
    "            output.write(\"\\n\")\n",
    "    output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "926506 926506 4000 4000 6328\n",
      "Logistic Regression\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "Random Forest\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "writePerformanceOutput()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
