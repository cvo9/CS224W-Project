{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/heathershen/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read train/test sets and node embeddings/features files\n",
    "def readFiles():\n",
    "    \n",
    "    # Read train/test sets\n",
    "    train_nodes = []\n",
    "    train_y = []\n",
    "    \n",
    "    test_nodes = []\n",
    "    test_y = []\n",
    "    \n",
    "    trainFile = \"train.tsv\"\n",
    "    testFile = \"test.tsv\"\n",
    "    \n",
    "    with open(trainFile) as new:\n",
    "        for line in csv.reader(new, delimiter=\"\\t\"):\n",
    "            train_nodes.append((int(line[0]), int(line[1])))\n",
    "            train_y.append(int(line[2]))\n",
    "    \n",
    "    with open(testFile) as new:\n",
    "        for line in csv.reader(new, delimiter=\"\\t\"):\n",
    "            test_nodes.append((int(line[0]), int(line[1])))\n",
    "            test_y.append(int(line[2]))\n",
    "    \n",
    "    # Read node embeddings file\n",
    "    node_embeddings = {} # dict from node ID to embedding\n",
    "    embeddingFile = \"train_edges.emb\"\n",
    "    \n",
    "    index = 0\n",
    "    with open(embeddingFile) as new:\n",
    "        for line in csv.reader(new, delimiter=\" \"):\n",
    "            # Ignore header\n",
    "            if index == 0:\n",
    "                index += 1\n",
    "                continue\n",
    "            \n",
    "            nodeID = int(line[0])\n",
    "            embeddings = np.array([float(val) for val in line[1:]])\n",
    "            \n",
    "            node_embeddings[nodeID] = embeddings\n",
    "            \n",
    "            index += 1\n",
    "            \n",
    "    # Read node network features files\n",
    "    node_network_features = {} # dict from node ID to network_features\n",
    "    networkFile = \"local_features_embeddings.txt\"\n",
    "    \n",
    "    index = 0\n",
    "    with open(networkFile) as new:\n",
    "        for line in csv.reader(new, delimiter=\"\\t\"):\n",
    "            # Ignore header\n",
    "            if index == 0:\n",
    "                index += 1\n",
    "                continue\n",
    "            \n",
    "            nodeID = int(line[0])\n",
    "            network_features = line[1].split(\" \")\n",
    "            network_features.remove(\"\")\n",
    "            network_features = np.array([float(val) for val in network_features])\n",
    "            \n",
    "            node_network_features[nodeID] = network_features\n",
    "            \n",
    "            index += 1\n",
    "    \n",
    "    # Read node chemical features files\n",
    "    node_fingerprints = {} # dict from node ID to fingerprints (for drug nodes)\n",
    "    fingerprintsFile = \"fingerprints.txt\"\n",
    "    \n",
    "    index = 0\n",
    "    with open(fingerprintsFile) as new:\n",
    "        for line in csv.reader(new, delimiter=\" \"):\n",
    "            # Ignore header\n",
    "            if index == 0:\n",
    "                index += 1\n",
    "                continue\n",
    "            \n",
    "            nodeID = int(line[0])\n",
    "            fingerprint = line[1:]\n",
    "            fingerprint.remove(\"\")\n",
    "            fingerprint = np.array([int(val) for val in fingerprint])\n",
    "            \n",
    "            node_fingerprints[nodeID] = fingerprint\n",
    "            \n",
    "            index += 1\n",
    "    \n",
    "    return train_nodes, train_y, test_nodes, test_y, node_embeddings, node_network_features, node_fingerprints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train/test set features from node embeddings, network features, and molecular fingerprints\n",
    "def createFeatures():\n",
    "    \n",
    "    train_nodes, train_y, test_nodes, test_y, node_embeddings, node_network_features, node_fingerprints = readFiles()\n",
    "    print(len(train_nodes), len(train_y), len(test_nodes), len(test_y), len(node_embeddings), len(node_network_features), len(node_fingerprints))\n",
    "    \n",
    "    # Features for each link include combining embeddings of the two nodes through Concatenation, Hadamard, Sum, Distance\n",
    "    # Additionally, for adding embeddings with network features/fingerprints, use concatenation\n",
    "    \n",
    "    feature_types = ['node2vec Concatenation', 'node2vec Hadamard Product', 'node2vec Sum', 'node2vec Distance',\n",
    "                    'node2vec + Network Features', 'node2vec + Molecular Fingerprints', 'node2vec + Network + Fingerprints']\n",
    "    train_x = [[] for i in range(len(feature_types))] # list of feature matrix for each type\n",
    "    test_x = [[] for i in range(len(feature_types))]\n",
    "    updated_train_y = []\n",
    "    updated_test_y = []\n",
    "    \n",
    "    # Combine node embeddings and features for train edges\n",
    "    for i in range(len(train_nodes)):\n",
    "        node1, node2 = train_nodes[i]\n",
    "        y = train_y[i]\n",
    "        node1_embed = node_embeddings[node1]\n",
    "        node2_embed = node_embeddings[node2]\n",
    "        node1_network = node_network_features[node1]\n",
    "        node2_network = node_network_features[node2]\n",
    "        \n",
    "        # Only use drugs with valid fingerprints\n",
    "        if node2 not in node_fingerprints:\n",
    "            continue\n",
    "            \n",
    "        node2_fingerprint = node_fingerprints[node2] # only drugs have molecular fingerprints\n",
    "        \n",
    "#         # node2vec Concatenate\n",
    "#         concat = np.concatenate((node1_embed, node2_embed))\n",
    "#         train_x[0].append(concat)\n",
    "        \n",
    "#         # node2vec Hadamard Product\n",
    "#         hadamard = np.multiply(node1_embed, node2_embed)\n",
    "#         train_x[1].append(hadamard)\n",
    "        \n",
    "#         # node2vec Sum\n",
    "#         summation = node1_embed + node2_embed\n",
    "#         train_x[2].append(summation)\n",
    "        \n",
    "#         # node2vec Absolute Distance\n",
    "#         distance = np.absolute(node1_embed - node2_embed)\n",
    "#         train_x[3].append(distance)\n",
    "        \n",
    "#         # node2vec + Network Features\n",
    "#         node2vec_network = np.concatenate((node1_network, node2_network))\n",
    "#         train_x[4].append(node2vec_network)\n",
    "        \n",
    "#         # node2vec + Molecular Fingerprints\n",
    "#         node2vec_fingerprint = np.concatenate((node1_embed, node2_embed, node2_fingerprint))\n",
    "#         train_x[5].append(node2vec_fingerprint)\n",
    "        \n",
    "        # node2vec + Network Features + Molecular Fingerprints\n",
    "        node2vec_network_fingerprint = np.concatenate((node1_network, node2_network, node2_fingerprint))\n",
    "        train_x[6].append(node2vec_network_fingerprint)\n",
    "        \n",
    "        # Add y\n",
    "        updated_train_y.append(y)\n",
    "    \n",
    "    # Combine node embeddings and features for test edges\n",
    "    for i in range(len(test_nodes)):\n",
    "        node1, node2 = test_nodes[i]\n",
    "        y = test_y[i]\n",
    "        node1_embed = node_embeddings[node1]\n",
    "        node2_embed = node_embeddings[node2]\n",
    "        node1_network = node_network_features[node1]\n",
    "        node2_network = node_network_features[node2]\n",
    "        \n",
    "        # Only use drugs with valid fingerprints\n",
    "        if node2 not in node_fingerprints:\n",
    "            continue\n",
    "            \n",
    "        node2_fingerprint = node_fingerprints[node2] # only drugs have molecular fingerprints\n",
    "        \n",
    "#         # node2vec Concatenate\n",
    "#         concat = np.concatenate((node1_embed, node2_embed))\n",
    "#         test_x[0].append(concat)\n",
    "        \n",
    "#         # node2vec Hadamard Product\n",
    "#         hadamard = np.multiply(node1_embed, node2_embed)\n",
    "#         test_x[1].append(hadamard)\n",
    "        \n",
    "#         # node2vec Sum\n",
    "#         summation = node1_embed + node2_embed\n",
    "#         test_x[2].append(summation)\n",
    "        \n",
    "#         # node2vec Absolute Distance\n",
    "#         distance = np.absolute(node1_embed - node2_embed)\n",
    "#         test_x[3].append(distance)\n",
    "        \n",
    "# #         node2vec + Network Features\n",
    "#         node2vec_network = np.concatenate((node1_network, node2_network))\n",
    "#         test_x[4].append(node2vec_network)\n",
    "        \n",
    "#         # node2vec + Molecular Fingerprints\n",
    "#         node2vec_fingerprint = np.concatenate((node1_embed, node2_embed, node2_fingerprint))\n",
    "#         test_x[5].append(node2vec_fingerprint)\n",
    "        \n",
    "#         # node2vec + Network Features + Molecular Fingerprints\n",
    "        node2vec_network_fingerprint = np.concatenate((node1_network, node2_network, node2_fingerprint))\n",
    "        test_x[6].append(node2vec_network_fingerprint)\n",
    "        \n",
    "        # Add y\n",
    "        updated_test_y.append(y)\n",
    "    \n",
    "    train_x = [np.array(matrix) for matrix in train_x]\n",
    "    test_x = [np.array(matrix) for matrix in test_x]\n",
    "    \n",
    "    train_y = updated_train_y\n",
    "    test_y = updated_test_y\n",
    "    \n",
    "    return train_x, train_y, test_x, test_y, feature_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict links using various classifier models; for each classifier, predict on each feature type and evaluate\n",
    "def predictClassifier():\n",
    "    train_x, train_y, test_x, test_y, feature_types = createFeatures()\n",
    "    \n",
    "    classifiers = ['Logistic Regression', 'Random Forest']\n",
    "    metrics = ['Accuracy', 'Precision', 'Recall', 'F1 Score']\n",
    "    \n",
    "    # list of performance metrics by feature type for each classifier\n",
    "    performance = [[] for i in range(len(classifiers))]\n",
    "    \n",
    "    # Logistic Regression\n",
    "    print (\"Logistic Regression\")\n",
    "    for i in range(6, 7): # already completed basic  4 node2vec features, start at 5th feature\n",
    "        print (i)\n",
    "        logReg = LogisticRegression()\n",
    "        \n",
    "        # Train model and predict\n",
    "        logReg.fit(train_x[i], train_y)\n",
    "        predictions = logReg.predict(test_x[i])\n",
    "        \n",
    "        # Evaluate performance metrics\n",
    "        accuracy = sklearn.metrics.accuracy_score(test_y, predictions)\n",
    "        precision = sklearn.metrics.precision_score(test_y, predictions)\n",
    "        recall = sklearn.metrics.recall_score(test_y, predictions)\n",
    "        f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "        \n",
    "        # Record metrics\n",
    "        performance[0].append((accuracy, precision, recall, f1_score))\n",
    "    \n",
    "    # Random Forest\n",
    "    print( \"Random Forest\")\n",
    "    for i in range(6, 7): # already completed basic  4 node2vec features, start at 5th feature\n",
    "        print( i)\n",
    "        rf = RandomForestClassifier()\n",
    "        \n",
    "        # Train model and predict\n",
    "        rf.fit(train_x[i], train_y)\n",
    "        predictions = rf.predict(test_x[i])\n",
    "        \n",
    "        # Evaluate performance metrics\n",
    "        accuracy = sklearn.metrics.accuracy_score(test_y, predictions)\n",
    "        precision = sklearn.metrics.precision_score(test_y, predictions)\n",
    "        recall = sklearn.metrics.recall_score(test_y, predictions)\n",
    "        f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "        \n",
    "        # Record metrics\n",
    "        performance[1].append([accuracy, precision, recall, f1_score])\n",
    "    \n",
    "    return classifiers, feature_types, metrics, performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write performance metrics to output file\n",
    "def writeClassifierPerformanceOutput():\n",
    "    classifiers, feature_types, metrics, performance = predictClassifier()\n",
    "    outputFileName = \"model_performance_all.txt\" # changed name to v2 for second run using network + fingerprint features\n",
    "    \n",
    "    with open(outputFileName, 'w') as output:\n",
    "        for i in range(len(classifiers)):\n",
    "            output.write(classifiers[i] + \":\\n\\n\")\n",
    "            for j in range(0,1):\n",
    "                output.write(feature_types[j] + \":\\n\")\n",
    "                for k in range(len(metrics)):\n",
    "                    output.write(metrics[k] + \": \" + str(performance[i][j][k]) + \"\\n\")\n",
    "                output.write(\"\\n\")\n",
    "            output.write(\"\\n\")\n",
    "    output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "926506 926506 4000 4000 6328 6328 1614\n"
     ]
    }
   ],
   "source": [
    "writeClassifierPerformanceOutput()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Predict nodes simply based on embedding distance\n",
    "def predictEmbedDistance():\n",
    "    train_nodes, train_y, test_nodes, test_y, node_embeddings = readFiles()\n",
    "    \n",
    "    # Obtain only real edges in train network\n",
    "    diseases = set([])\n",
    "    drugs = set([])\n",
    "    positive_train_nodes = set([])\n",
    "    positive_test_nodes = set([])\n",
    "    \n",
    "    # Train nodes\n",
    "    for i in range(len(train_nodes)):\n",
    "        disease, drug = train_nodes[i]\n",
    "        \n",
    "        if train_y[i] != 1:\n",
    "            continue\n",
    "        \n",
    "        diseases.add(disease)\n",
    "        drugs.add(drug)\n",
    "        positive_train_nodes.add((disease, drug))\n",
    "    \n",
    "    # Test nodes\n",
    "    for i in range(len(test_nodes)):\n",
    "        disease, drug = test_nodes[i]\n",
    "        \n",
    "        if test_y[i] != 1:\n",
    "            continue\n",
    "        \n",
    "        diseases.add(disease)\n",
    "        drugs.add(drug)\n",
    "        positive_test_nodes.add((disease, drug))\n",
    "        \n",
    "    print len(positive_train_nodes), len(positive_test_nodes), len(diseases), len(drugs)\n",
    "    \n",
    "    # Calculate L2 distance between each disease drug\n",
    "    distances = []\n",
    "    \n",
    "    for disease in diseases:\n",
    "        for drug in drugs:\n",
    "            if (disease, drug) not in positive_train_nodes:\n",
    "                distance = np.linalg.norm(node_embeddings[disease] - node_embeddings[drug])\n",
    "                distances.append((distance, disease, drug))\n",
    "    distances.sort()\n",
    "    print distances[:5]\n",
    "    \n",
    "    # Sort by by ascending order, take top n as newly predicted links, count how many are \"correct\" (in test set)\n",
    "    n = len(positive_test_nodes)\n",
    "    num_correct = 0\n",
    "    \n",
    "    for distance, disease, drug in distances[:n]:\n",
    "        if (disease, drug) in positive_test_nodes:\n",
    "            num_correct += 1\n",
    "            \n",
    "    accuracy = float(num_correct) / n\n",
    "    print \"Predict Links via Embedding Distance Accuracy:\", accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "463253 2000 4766 1562\n",
      "[(0.1085827633657915, 12488, 602013), (0.11357551461669167, 12585, 600895), (0.11476520661489824, 12510, 603629), (0.11595785678059664, 12213, 603629), (0.11712053956092647, 7871, 605212)]\n",
      "Predict Links via Embedding Distance Accuracy: 0.0\n"
     ]
    }
   ],
   "source": [
    "predictEmbedDistance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
